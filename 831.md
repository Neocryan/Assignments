## Finished
* benchmark of the models: Infersent(without self-attention), Universal(from Google, tensorflow based, hard to extract weights?),
SSE(three version: original, w2v-tuned, all-over-tuned)
* Collected enough test data. 
* An api from infersent with its model in the datalake. (For Pengfei, and also can be used for others)

## In progess
* a w2v-infersent-cosine tune for Quora data, still training. (1000 epochs expected)
* Loss function of ranking probability.
* Another structure to deal with semantic similarity (image[])

## Planning
* Benchmark for the test data (how to define acc)
* Reinforcement System (loss, grad, back propa)
* Think about how to tune different part of network : w2v(embedding layer)? All the parameters? 

## Clearify
* what is the object: To build a universal sentence embedding? Or trying to transfer the generic model to a specific scope.
* The baseline for the accuracy and latency. 
